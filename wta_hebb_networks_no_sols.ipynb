{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badc90e0-fb90-4bef-a7d8-8cba534489d4",
   "metadata": {},
   "source": [
    "## Hebbian learning in feedforward networks.\n",
    "\n",
    "Based on the code below, implement a number of experiments to investigate how Hebbian learning behaves when training neural networks.\n",
    "To be able to evaluate how successful an approach is, we will be using a subset of the MNIST dataset to train on and evaluate the quality of the learned Hebbian features by using either a \"majority-vote\" readout or a linear regression to finish the classification problem (see below). Additionally, we can look at the weight vectors as images to get a sense of the quality of the learned, unsupervised features.\n",
    "\n",
    "Note, that the network we are using is completely linear and has no biases. $y = W x$ where $y$ is the output and $x$ is the input.\n",
    "\n",
    "Your job will be to implement the training of these features (on batches and for networks!), the rest of the code is provided.\n",
    "\n",
    "For each of the variants below, discuss the outcomes for each of the evaluation steps. Use a markdown cell at the bottom of the notebook, and compare the results to each other. First, write down the hypothesis for the experiments, then discuss what you have learned.\n",
    "\n",
    "Here are the variants to compare:\n",
    "\n",
    "1. a standard hebbian learning rule, for one neuron: $w_i^{n+1} = w_i^{n} + \\eta  x_i y$ where $y$ is the non-linear output, $x$ is the input, and $i$ gives the input index. $\\eta$ is the learning rate. $n$ is the bacht-step.\n",
    "2. a standard hebbian learning, but only the neuron with the highest activation (the winner) is allowed to learn.\n",
    "3. a standard hebbian learning rule + WTA with weight normalization.\n",
    "4. use Oja's rule instead of a standard hebbian rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2423a",
   "metadata": {},
   "source": [
    "1. If units start responding to the same features, the vote evaluation will have an issue determining the most active unit. A linear classifier can only learn from the output if hebbian learning is stopped before learning to respond to the same features, if it has learned to identify units before that (while some randomness is still in the network).\n",
    "\n",
    "2. A working WTA approach should work well with the voting evaluation and should work with a linear classifier, but the weights may increase a lot\n",
    "\n",
    "3. Same as above but without exploding weights\n",
    "\n",
    "4. Same as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3685b204-c8e1-4878-8d2a-c531177caa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4295c47e-b9ee-48d2-a805-51e2ae697205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_numpy(root):\n",
    "    root = Path(root)\n",
    "    train_ds = datasets.MNIST(root, train=True, download=True)\n",
    "    test_ds  = datasets.MNIST(root, train=False, download=True)\n",
    "\n",
    "    def ds_to_numpy(ds):\n",
    "        images = np.stack([np.array(img, dtype=np.float32).reshape(-1) / 255.0 for img, _ in ds])\n",
    "        labels = np.fromiter((label for _, label in ds), dtype=np.int32, count=len(ds))\n",
    "        return images, labels\n",
    "\n",
    "    Xtr, ytr = ds_to_numpy(train_ds)\n",
    "    Xte, yte = ds_to_numpy(test_ds)\n",
    "    return Xtr, ytr, Xte, yte\n",
    "\n",
    "\n",
    "def make_subset(X: np.ndarray, y: np.ndarray, digits: Sequence[int], fraction: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mask = np.isin(y, digits)\n",
    "    idx = np.where(mask)[0]\n",
    "    rng = np.random.default_rng(0)\n",
    "    rng.shuffle(idx)\n",
    "    k = int(len(idx) * fraction)\n",
    "    idx = idx[:k]\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f7047a-d27f-4ed9-aa30-e8ca0bbff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "digits = [0,1,2,3,4] # we are only going to look at 5 classes for now\n",
    "fraction_of_dataset = 0.5 # and only for 50% of the images in there\n",
    "\n",
    "\n",
    "Xtr_all, ytr_all, Xte_all, yte_all = load_mnist_numpy(data_dir) # full dataset\n",
    "Xtr, ytr = make_subset(Xtr_all, ytr_all, digits, fraction_of_dataset) # subset training\n",
    "Xte, yte = make_subset(Xte_all, yte_all, digits, fraction_of_dataset) # subset testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13edfa07-c930-49a2-9d57-3195ae45bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WTANet:\n",
    "\n",
    "    def __init__(self, input_dim: int, n_units: int = 64, rule: str = \"oja\", WTA: bool = True):\n",
    "        assert rule in (\"oja\", \"hebb\"), \"rule must be 'oja' or 'hebb'\"\n",
    "        self.rule = rule\n",
    "        self.WTA = WTA\n",
    "        self.rng  = np.random.default_rng()\n",
    "        self.W    = self.rng.normal(0, 0.1, size=(n_units, input_dim)).astype(np.float32)\n",
    "        self.W   /= np.linalg.norm(self.W, axis=1, keepdims=True) + 1e-9\n",
    "\n",
    "    # Your job: Forward + learning\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        # implement the forward pass for the hebbain activations.\n",
    "        output = X @ self.W.T  # (B, H)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def update_weights_batch(self, X: np.ndarray, eta: float):\n",
    "        # implement the hebbian learning rules (and with winner take all). Do this for a whole batch at a time!\n",
    "        # you can switch between different rules based on self.rule\n",
    "        if self.rule == \"hebb\":\n",
    "            if self.WTA: \n",
    "                # Apply winner-take-all: set all but the max activation to 0\n",
    "                max_activated = output.max(axis=1, keepdims=True)\n",
    "                output = np.where(output == max_activated, output, 0.0) # do hebbian learning update only for the winner then\n",
    "            else:\n",
    "                y = self.forward(X)       \n",
    "\n",
    "        \n",
    "        elif self.rule == \"oja\":\n",
    "            pass\n",
    "\n",
    "        delta = eta * (X.T @ y)  \n",
    "        self.W += delta.T  # (H, D)\n",
    "\n",
    "    # ----- helpers ------\n",
    "    \n",
    "    # Supervised linear read‑out (ridge regression)\n",
    "    def train_linear_classifier(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-3):\n",
    "        \"\"\"Fit `W_out` via linear regression. This is technically a \"delta rule\".\n",
    "        \"\"\"\n",
    "        A = self.forward(X)                 # (N, H)\n",
    "        N, H = A.shape\n",
    "        classes = int(y.max()) + 1\n",
    "        # One‑hot encode labels → Y (N, C)\n",
    "        Y = np.zeros((N, classes), dtype=np.float32)\n",
    "        Y[np.arange(N), y] = 1.0\n",
    "        # Closed‑form ridge solution\n",
    "        I = np.eye(H, dtype=np.float32)\n",
    "        self.W_out = np.linalg.solve(A.T @ A + reg * I, A.T @ Y)  # (H, C), safe in the object\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return predicted class indices using the trained linear read‑out.\"\"\"\n",
    "        if self.W_out is None:\n",
    "            raise RuntimeError(\"Linear classifier not trained. Call train_linear_classifier first.\")\n",
    "        logits = self.forward(X) @ self.W_out  # (B, C)\n",
    "        return logits.argmax(axis=1)\n",
    "\n",
    "    def linear_accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Accuracy using the supervised read‑out (must be trained).\"\"\"\n",
    "        preds = self.predict(X)\n",
    "        return float((preds == y).mean())\n",
    "\n",
    "    # Evaluation with majority votes.\n",
    "    def majority_labels(self, X: np.ndarray, y: np.ndarray) -> list[int]:\n",
    "        winners = self.forward(X).argmax(axis=1)\n",
    "        bag = [[] for _ in range(self.W.shape[0])]\n",
    "        for w, lbl in zip(winners, y):\n",
    "            bag[w].append(lbl)\n",
    "        return [max(set(b), key=b.count) if b else -1 for b in bag]\n",
    "\n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray, unit_labels: Sequence[int]) -> float:\n",
    "        preds = np.take(unit_labels, self.forward(X).argmax(axis=1))\n",
    "        return float((preds == y).mean())\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "# Your job: Training loop\n",
    "def train(net: WTANet, X: np.ndarray, epochs: int, eta: float, batch_size: int):\n",
    "    # train on batches for multiple epochs. \n",
    "    # its a good idea to randomize the order of training per epoch.\n",
    "    for e in range(epochs):\n",
    "        for Xb in batch(X, batch_size):\n",
    "            net.update_weights_batch(Xb, eta)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964f7aca-7808-449f-843b-c470b46c6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64\n",
    "rule = \"hebb\" # \"hebb\" and \"oja\", are your options here\n",
    "WTA = False # whether to use winner-take-all or not\n",
    "\n",
    "epochs = 3\n",
    "eta = 0.05 # learning rate\n",
    "batch_size = 64 # batch size\n",
    "\n",
    "# input dim is just MNIST size.\n",
    "net = WTANet(input_dim=784, n_units=hidden_units, rule=rule, WTA=WTA)\n",
    "train(net, Xtr, epochs=epochs, eta=eta, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b76633e-7aa4-4586-8560-53e601a05863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_vectors(W, cols=8, figsize_ratio=1.2, cmap=\"gray\"):\n",
    "    n_units, dim = W.shape\n",
    "    assert dim == 28 * 28, \"Weights must be flattened 28×28 images\"\n",
    "\n",
    "    rows = int(np.ceil(n_units / cols))\n",
    "    plt.figure(figsize=(cols * figsize_ratio, rows * figsize_ratio))\n",
    "    for i in range(rows * cols):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        if i < n_units:\n",
    "            plt.imshow(W[i].reshape(28, 28), cmap=cmap)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa5d747-553c-47c6-9933-d5bc82a780d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x960 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the hebbian weights as images.\n",
    "\n",
    "plot_weight_vectors(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58cc5e5-36c3-46e7-b5d1-c7ff70723cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.221\n"
     ]
    }
   ],
   "source": [
    "#  Evaluate a simple \"majory label\" approach: \n",
    "#  i) Every neuron gets a vote according to the input class it is most active for\n",
    "# ii) For every input, every neuron votes for that class, majority wins.\n",
    "\n",
    "unit_lbls = net.majority_labels(Xtr, ytr)\n",
    "acc       = net.accuracy(Xte, yte, unit_lbls)\n",
    "\n",
    "print(f\"Test accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50026f2-f3ea-499a-9cf6-558eceb761fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.193\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, evaluate a simple linear classifier on the features learned by the hebb algorith,.\n",
    "\n",
    "net.train_linear_classifier(Xtr, ytr)\n",
    "acc_lc = net.linear_accuracy(Xte, yte)\n",
    "\n",
    "print(f\"Test accuracy: {acc_lc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85af213",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "1. Standard hebbian learning: voting eval almost random (0.221), lin. classifier slightly lower (0.193)\n",
    "\n",
    "2. WTA hebbian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df030e66-7bc9-4956-861f-688e925c04c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
